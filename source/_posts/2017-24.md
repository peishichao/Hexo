---
title: 轻量级爬虫-百度百科页面抓取
date: 2017-02-20 21:30:31
categories: 技术分享
tags:
- 轻量级
- 爬虫
- 百度百科
- python3写文件编码格式问题
---

#### 轻量级爬虫

![Markdown](http://i1.piimg.com/1949/581edab5a109c854.png)

<!--more-->

*爬虫工具：*URL管理器，网页下载器urllib2，网页解析器BeautifulSoup

*目标任务：*爬去百度百科Python词条相关的1000个页面数据

> 爬虫：一段自动抓取互联网信息的程序
>

**价值：互联网数据，为我所用**

#### 简单的爬虫架构:

![Markdown](http://i1.piimg.com/1949/735d4e1aa7b07375.png)

***爬虫调度端：***URL管理器-网页下载器-网页解析器-价值数据

***URL管理器：***管理待抓取URL集合和已抓取URL集合

- 防止重复抓取、防止循环抓取

*实现方式：*

![Markdown](http://i1.piimg.com/1949/10b3b5b6501f5d3b.png)

> 将URL数据放置在内存中，
>
> 将URL数据放置在关系数据库中，
>
> 将URL数据放在缓存数据库中

![Markdown](http://i1.piimg.com/1949/a37cdd37f3433a86.png)

***网页下载器：***将互联网上URL对应的网页下载到本地的工具

![Markdown](http://i1.piimg.com/1949/810128d9f55b505a.png)

Python网页下载器分类：

- urllib2-Python官方基础模块


​	三种方式：

```python
#python2实例代码如需python3请转至此页最后查看源码，源码由python3编写
#1.urllib2.urlopen(url)最简洁的方法

import urllib2
#直接请求
response = urllib2.urlopen('http://www.baidu.com')
#获取状态码，如果是200表示获取成功
print response.getcode()
#读取内容
cont = response.read()

#2.添加data（向服务器提交需要用户输入的数据）、http header

import urllib2
#创建Request对象
request = urllib2.Request(url)
#添加数据
request.add_data('a','1')#需要用户输入的数据
#添加http的header
request.add_header('User-Agent','Mozilla/5.0')#模拟伪装浏览器
#发送请求获取结果
response = urllib2.urlopen(request)

#3.添加特殊情景的处理器
#HTTPCookieProcessor(需登录) ProxyHandler (需代理)
#HTTPSHandler(HTTPS加密访问) HTTPRedirectHandler(自动跳转URL)

import urllib2,cookielib
#创建cookie容器
cj = cookielib.CookieJar()
#创建一个opener
opener = urllib2.build_opener(urllib2.HTTPCookieProcessor(cj))
#给urllib2安装opener
urllib2.install_opener(opener)
#使用带有cookie的urllib2访问网页
response = urllib2.urlopen("http://www.baidu.com/")
```

- requests-第三方包更强大

特殊情景的处理器介绍：

![Markdown](http://i1.piimg.com/1949/ad0cad3615ea4a4c.png)

***网页解析器***

网页解析器：从网页中提取有价值数据的工具

![Markdown](http://p1.bpimg.com/1949/024f2085649e50d9.png)

python网页解析器的分类：

正则表达式、html.parser、BeautifulSoup、lxml

![Markdown](http://p1.bpimg.com/1949/01a626ce7a5ca361.png)

![Markdown](http://p1.bpimg.com/1949/8c7f4d15405149b5.png)

结构化解析-DOM树

![Markdown](http://p1.bpimg.com/1949/dc56a3e88394d5f1.png)

[BeautifulSoup安装](https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html)

> 如果你用的是新版的Debain或ubuntu,那么可以通过系统的软件包管理来安装:
>
> $ apt-get install Python-bs4
>
> Beautiful Soup 4 通过PyPi发布,所以如果你无法使用系统包管理安装,那么也可以通过 easy_install 或 pip 来安装.包的名字是 beautifulsoup4 ,这个包兼容Python2和Python3.
>
> $ easy_install beautifulsoup4
>
> $ pip install beautifulsoup4

Beautiful Soup网页解析器语法

![Markdown](http://p1.bpimg.com/1949/e212eaf402ecaa75.png)

创建BeautifulSoup对象

```python
from bs4 import BeautifulSoup
import re
#根据HTML网页字符串创建BeautifulSoup对象
soup = BeautifulSoup(
					html_doc,			#HTML文档字符串
    				'html.parser'		 #文档解析器
    				from_encoding = 'utf8'#HTML文档的编码
					)
#方法：find_all(name,attr,string)
#查找所有标签为a的节点
soup.find_all('a')
#查找所有标签为a,链接符合/view/123.htm形式的节点
soup.find_all('a',href='/view/123.htm')
soup.find_all('a',href=re.compile(r'/view/d+\.htm'))
#查找所有标签为div,class为adc,文字为Python的节点
soup.find_all('div',class_=='abc',string='Python')
#得到节点<a href='1.html'>Python</a>
#获取查找到的节点的标签名称
naode.name
#获取查找到的a节点的href属性
node['href']
#获取查找到的a节点的链接文字
node.get_text()
```

***实例爬虫：***（目标网站可能随时升级其网站策略所以抓取策略必须具备及时性）

![Markdown](http://p1.bpimg.com/1949/4c4055703303a3b4.png)

***确定目标：***哪些网站的哪些网页的那些数据

（百度百科Python词条相关词条网页-标题和简介）

***入口页：***http://baike.baidu.com/item/Python http://baike.baidu.com/view/21087.htm

***分析目标：***URL格式+数据格式+网页编码

***URL格式：***

- 词条页面URL: /view/12345.htm

***数据格式：***

标题

- <dd class="lemmaWgt-lemmaTitle-title">
  <h1>Python</h1>
  </dd>

简介

- <div class="lemma-summary" label-module="lemmaSummary">

  ​
  </div>

页面编码：UTF-8

编码代码

执行爬虫

源码地址:https://github.com/peishichao/-

该源码由python3编写，测试通过。

python写文件编码问题：

```python
def output_html(self):
        fout = open('outputer_html', 'w', encoding='utf-8')

        fout.write('<html>')
        fout.write("<head><meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\"></head>")
        fout.write('<body>')
        fout.write('<table>')

        # ascii python 默认编码
        for data in self.datas:
            fout.write('<tr>')
            fout.write('<td>%s</td>' % data['url'])
            fout.write('<td>%s</td>' % data['title'])
            fout.write('<td>%s</td>' % data['summary'])
            fout.write('</tr>')

        fout.write('</table>')
        fout.write('</body>')
        fout.write('</html>')

        fout.close()
```

经测试在windows平台下python3运行环境下上述方式不会出现乱码现象。直接在打开文件的时候声明encode编码格式为*utf-8*