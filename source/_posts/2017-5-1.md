---
title: 解决github pages屏蔽百度爬虫的问题
date: 2017-01-18 22:48:34
categories: 技术分享
tags:
- github pages
- 屏蔽
- 百度爬虫
---

![Markdown](http://p1.bqimg.com/1949/82bd234fbaaa83e4.png)

<!--more-->

GitHub pages屏蔽了百度爬虫(屏蔽了百度爬虫的UAMozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html))。这样你的博客如果是部署在github上面的话是很难被百度收录的，从百度站长工具抓取诊断中就可以发现爬取都是403 forbidden。

```
HTTP/1.0 403 Forbidden
Cache-Control: no-cache
Connection: close
Content-Type: text/html
```

目前主要有两种解决方案：CDN回源和为百度spider指定特殊线路。

### CDN回源

博客一般都是静态文件，很适合使用CDN回源，这样不仅能提高访问速度还能跳过github对百度爬虫的屏蔽。
回源的意思是用户请求hexo之后CDN会抓取博客内容，这样其他用户就不用访问github pages了而是直接从CDN就近的节点拉取数据，访问速度更快。但是这里有个问题，比如百度爬虫所在的区域CDN节点还没有缓存博客数据，这样百度爬虫会穿透CDN直接请求到github pages，然后后果就是github pages返回403 forbidden。所以CDN回源的方式仅适用于博客访客量比较大的情况（这样访问会在百度爬虫之前让CDN缓存博客数据，当然也只是比较大的概率）。

不过如果无视百度爬虫，给hexo加上CDN回源也是很不错的，至少能提高访问速度。我使用的是百度云加速。百度云加速提供免费的CDN加速功能，还可以更快的被百度蜘蛛抓到，何乐而不为呐。部分操作可以参考我的这篇博客的相关提示。[本博客的SEO优化（2）-让谷歌和百度尽快收录你](http://www.steven7.top/2016/08/15/Hexo-Seo-2/)

### 为百度spider指定特殊线路

除了github pages能托管hexo之外，还有其他的选择，比如gitcafe pages(已经和coding.net合并)。传送门：[gitcafe pages配置](https://coding.net/help/doc/pages/index.html)，配置还是挺简单的。而且gitcafe没有屏蔽百度爬虫。但是gitcafe pages的服务器目前还不够github给力（偶尔出现一些无法打开的情况），所以有了新的思路：

正常用户访问github pages，百度爬虫访问gitcafe

现在的DNS解析服务一般都可以指定特定的线路，比如指定百度爬虫的线路解析到gitcafe。

